import os
import uuid
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import Any, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
from dotenv import load_dotenv
import stripe

from apex_engine import analyse_cybersec
from pdf_generator import generate_pdf_report

# Chargement des variables d'environnement
load_dotenv()

# STRIPE
STRIPE_SECRET_KEY = os.getenv("STRIPE_SECRET_KEY")
STRIPE_PRICE_24H = os.getenv("STRIPE_PRICE_24H", "price_XXX")
STRIPE_PRICE_48H = os.getenv("STRIPE_PRICE_48H", "price_YYY")
STRIPE_SUCCESS_URL = os.getenv("STRIPE_SUCCESS_URL", "https://velnor.fr/success")
STRIPE_CANCEL_URL = os.getenv("STRIPE_CANCEL_URL", "https://velnor.fr/cancel")

# Logger global
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
logger = logging.getLogger("velnor")

# App
app = FastAPI(title="VELNOR API", version="2.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origin_regex=r"https://.*\.?velnor\.fr",
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["Content-Type", "Authorization", "Access-Control-Allow-Origin"]
)

if not STRIPE_SECRET_KEY:
    logger.error("‚ö†Ô∏è STRIPE_SECRET_KEY non d√©fini dans .env")
stripe.api_key = STRIPE_SECRET_KEY

# R√©pertoires
BASE_DIR = Path(__file__).resolve().parent
REPORTS_DIR = BASE_DIR / "rapports"
REPORTS_DIR.mkdir(exist_ok=True)
LOGS_DIR = BASE_DIR / "logs"
LOGS_DIR.mkdir(exist_ok=True)

app.mount("/rapports", StaticFiles(directory=REPORTS_DIR), name="rapports")

class ScanRequest(BaseModel):
    url: str

class ScanResponse(BaseModel):
    status: str
    task_id: str
    message: str

@app.get("/")
def root() -> Dict[str, str]:
    return {"message": "VELNOR backend op√©rationnel v2.0", "status": "running"}

@app.get("/health")
def health_check() -> Dict[str, str]:
    """Endpoint de sant√© pour v√©rifier que le service fonctionne"""
    return {"status": "healthy", "timestamp": datetime.utcnow().isoformat()}

# üÜï NOUVELLE ROUTE POUR T√âL√âCHARGER LES PDFs
@app.get("/download-pdf/{task_id}")
async def download_pdf(task_id: str):
    """
    T√©l√©charge le rapport PDF d'un audit cybers√©curit√©
    """
    try:
        logger.info(f"[{task_id}] üìÑ Demande de t√©l√©chargement PDF")
        
        # Chemin du PDF
        pdf_path = REPORTS_DIR / task_id / "audit.pdf"
        
        # V√©rifier si le fichier existe
        if not pdf_path.exists():
            logger.error(f"[{task_id}] ‚ùå PDF non trouv√©: {pdf_path}")
            raise HTTPException(status_code=404, detail="Rapport PDF non trouv√©")
        
        # Nom du fichier pour le t√©l√©chargement
        filename = f"velnor_audit_{task_id}.pdf"
        
        logger.info(f"[{task_id}] ‚úÖ PDF trouv√©, envoi du fichier")
        
        return FileResponse(
            path=str(pdf_path),
            filename=filename,
            media_type='application/pdf',
            headers={
                "Content-Disposition": f"attachment; filename={filename}",
                "Content-Type": "application/pdf"
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[{task_id}] ‚ùå Erreur t√©l√©chargement PDF: {str(e)}")
        raise HTTPException(status_code=500, detail="Erreur lors du t√©l√©chargement du PDF")

# üÜï ROUTE ALTERNATIVE POUR V√âRIFIER SI LE PDF EXISTE
@app.get("/pdf-ready/{task_id}")
async def check_pdf_ready(task_id: str) -> Dict[str, Any]:
    """
    V√©rifie si le PDF est pr√™t pour t√©l√©chargement
    """
    try:
        pdf_path = REPORTS_DIR / task_id / "audit.pdf"
        
        if pdf_path.exists():
            file_size = pdf_path.stat().st_size
            return {
                "ready": True,
                "task_id": task_id,
                "file_size": file_size,
                "download_url": f"/download-pdf/{task_id}"
            }
        else:
            return {
                "ready": False,
                "task_id": task_id,
                "message": "PDF en cours de g√©n√©ration"
            }
            
    except Exception as e:
        logger.error(f"[{task_id}] Erreur v√©rification PDF: {str(e)}")
        return {
            "ready": False,
            "task_id": task_id,
            "error": str(e)
        }

# ThreadPoolExecutor global avec gestion d'erreurs am√©lior√©e
executor = ThreadPoolExecutor(max_workers=3, thread_name_prefix="velnor_scan")

def validate_url(url: str) -> tuple[bool, str]:
    """Valide et normalise une URL"""
    try:
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        # Test de connectivit√© basique
        resp = requests.head(url, timeout=10, allow_redirects=True)
        if resp.status_code >= 400:
            return False, f"URL retourne status {resp.status_code}"
        
        return True, url
    except requests.exceptions.ConnectionError:
        return False, "Impossible de se connecter √† l'URL"
    except requests.exceptions.Timeout:
        return False, "Timeout lors de la connexion"
    except requests.exceptions.InvalidURL:
        return False, "Format d'URL invalide"
    except Exception as e:
        return False, f"Erreur validation URL: {str(e)}"

def execute_cybersec_analysis(url: str, task_id: str, audit_dir: Path) -> None:
    """
    Ex√©cute l'analyse cybers√©curit√© dans un thread s√©par√© avec gestion d'erreurs compl√®te
    """
    json_path = audit_dir / "result.json"
    pdf_path = audit_dir / "audit.pdf"
    
    # Logger sp√©cifique pour cette t√¢che
    task_logger = logging.getLogger(f"velnor.task.{task_id}")
    task_logger.setLevel(logging.INFO)
    
    # √âviter les doublons de handlers
    if not task_logger.handlers:
        task_log_path = LOGS_DIR / f"main_logs_{task_id}.txt"
        file_handler = logging.FileHandler(task_log_path)
        formatter = logging.Formatter("%(asctime)s [MAIN] %(levelname)s %(message)s")
        file_handler.setFormatter(formatter)
        task_logger.addHandler(file_handler)
    
    start_time = datetime.utcnow()
    
    try:
        task_logger.info(f"=== D√âBUT THREAD ANALYSE - Task ID: {task_id} ===")
        task_logger.info(f"URL: {url}")
        task_logger.info(f"R√©pertoire audit: {audit_dir}")
        
        # Phase 1: Validation URL approfondie
        task_logger.info("Phase 1: Validation URL approfondie...")
        try:
            resp = requests.get(url, timeout=15, verify=False)
            resp.raise_for_status()
            task_logger.info(f"URL accessible - Status: {resp.status_code}")
        except Exception as e:
            error_msg = f"URL non accessible: {str(e)}"
            task_logger.error(error_msg)
            
            # R√©sultat d'erreur minimal
            result = {
                "status": "failed",
                "task_id": task_id,
                "url": url,
                "error": error_msg,
                "score": 0,
                "niveau_risque": "Erreur",
                "resume": "‚ùå URL inaccessible",
                "anomalies": [error_msg],
                "recommendations": ["V√©rifier que l'URL est correcte et accessible"],
                "failles": [],
                "failures": [{"phase": "url_validation", "type": "ConnectionError", "message": error_msg}],
                "phase_results": {"url_validation": {"status": "failed", "error": error_msg}},
                "details_score": {"total": 0},
                "statistiques": {
                    "nb_anomalies": 1,
                    "nb_recommendations": 1,
                    "nb_failles": 0,
                    "nb_failures": 1,
                    "phases_success": 0,
                    "phases_failed": 1,
                    "duree_analyse": (datetime.utcnow() - start_time).total_seconds(),
                    "timestamp": datetime.utcnow().isoformat()
                },
                "metadata": {
                    "version": "2.0",
                    "engine": "velnor_main",
                    "log_file": f"logs/main_logs_{task_id}.txt"
                }
            }
            
            with open(json_path, "w", encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            task_logger.info("R√©sultat d'erreur sauvegard√©")
            return

        # Phase 2: Analyse cybers√©curit√© avec notre engine robuste
        task_logger.info("Phase 2: Lancement analyse cybers√©curit√©...")
        
        # Appel √† notre fonction ultra-robuste
        result_data = analyse_cybersec(url, task_id)
        
        task_logger.info(f"Analyse termin√©e - Status: {result_data.get('status', 'unknown')}")
        task_logger.info(f"Score obtenu: {result_data.get('score', 0)}/100")
        task_logger.info(f"Failles d√©tect√©es: {len(result_data.get('failles', []))}")
        
        # Phase 3: G√©n√©ration PDF
        task_logger.info("Phase 3: G√©n√©ration du rapport PDF...")
        
        try:
            generate_pdf_report(result_data, str(pdf_path))
            task_logger.info(f"PDF g√©n√©r√© avec succ√®s: {pdf_path}")
            pdf_generated = True
        except Exception as e:
            task_logger.error(f"Erreur g√©n√©ration PDF: {str(e)}")
            pdf_generated = False
            result_data["pdf_error"] = str(e)

        # Phase 4: Finalisation et sauvegarde
        finished_time = datetime.utcnow()
        duration = (finished_time - start_time).total_seconds()
        
        # Enrichissement des m√©tadonn√©es
        result_data.update({
            "pdf": f"/download-pdf/{task_id}" if pdf_generated else None,  # üÜï Nouvelle URL
            "pdf_generated": pdf_generated,
            "started_at": start_time.isoformat(),
            "finished_at": finished_time.isoformat(),
            "duration_seconds": duration
        })
        
        # Mise √† jour des statistiques si elles existent
        if "statistiques" in result_data:
            result_data["statistiques"]["duration_total"] = duration
            result_data["statistiques"]["pdf_generated"] = pdf_generated
        
        # Sauvegarde du r√©sultat
        with open(json_path, "w", encoding='utf-8') as f:
            json.dump(result_data, f, indent=2, ensure_ascii=False)
        
        task_logger.info(f"=== ANALYSE TERMIN√âE AVEC SUCC√àS ===")
        task_logger.info(f"Dur√©e totale: {duration:.2f}s")
        task_logger.info(f"Fichiers g√©n√©r√©s: JSON ‚úì, PDF {'‚úì' if pdf_generated else '‚úó'}")
        
    except Exception as e:
        # Gestion d'erreur de dernier recours
        task_logger.exception(f"ERREUR CRITIQUE DANS LE THREAD: {str(e)}")
        
        error_result = {
            "status": "error",
            "task_id": task_id,
            "url": url,
            "error": f"Erreur critique: {str(e)}",
            "score": 0,
            "niveau_risque": "Erreur critique",
            "resume": "‚ùå Erreur syst√®me lors de l'analyse",
            "anomalies": [f"Erreur syst√®me: {str(e)}"],
            "recommendations": ["Contacter le support technique"],
            "failles": [],
            "failures": [{"phase": "system", "type": "CriticalError", "message": str(e)}],
            "phase_results": {},
            "details_score": {},
            "statistiques": {
                "nb_anomalies": 1,
                "nb_recommendations": 1,
                "nb_failles": 0,
                "nb_failures": 1,
                "phases_success": 0,
                "phases_failed": 1,
                "duree_analyse": (datetime.utcnow() - start_time).total_seconds(),
                "timestamp": datetime.utcnow().isoformat()
            },
            "metadata": {
                "version": "2.0",
                "engine": "velnor_main_error",
                "log_file": f"logs/main_logs_{task_id}.txt"
            }
        }
        
        try:
            with open(json_path, "w", encoding='utf-8') as f:
                json.dump(error_result, f, indent=2, ensure_ascii=False)
        except Exception as save_error:
            task_logger.error(f"Impossible de sauvegarder le r√©sultat d'erreur: {save_error}")
    
    finally:
        # Nettoyage des handlers pour √©viter les fuites m√©moire
        for handler in task_logger.handlers[:]:
            handler.close()
            task_logger.removeHandler(handler)

@app.post("/scan", response_model=ScanResponse)
def scan_url(data: ScanRequest) -> ScanResponse:
    """
    Endpoint principal pour lancer un audit cybers√©curit√©
    Version ultra-robuste avec gestion d'erreurs compl√®te
    """
    try:
        # G√©n√©ration task_id unique
        task_id = uuid.uuid4().hex[:8]
        
        logger.info(f"[{task_id}] üì• Nouvelle demande d'audit re√ßue")
        logger.info(f"[{task_id}] üéØ URL demand√©e: {data.url}")
        
        # Validation pr√©liminaire de l'URL
        is_valid, validated_url_or_error = validate_url(data.url)
        if not is_valid:
            logger.error(f"[{task_id}] ‚ùå URL invalide: {validated_url_or_error}")
            return ScanResponse(
                status="error",
                task_id=task_id,
                message=f"URL invalide: {validated_url_or_error}"
            )
        
        validated_url = validated_url_or_error
        logger.info(f"[{task_id}] ‚úÖ URL valid√©e: {validated_url}")
        
        # Cr√©ation du r√©pertoire d'audit
        audit_dir = REPORTS_DIR / task_id
        audit_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"[{task_id}] üìÅ R√©pertoire cr√©√©: {audit_dir}")
        
        # Soumission du travail au ThreadPoolExecutor
        try:
            future = executor.submit(execute_cybersec_analysis, validated_url, task_id, audit_dir)
            logger.info(f"[{task_id}] üöÄ Thread d'analyse soumis avec succ√®s")
            
            return ScanResponse(
                status="en_cours",
                task_id=task_id,
                message=f"Analyse cybers√©curit√© d√©marr√©e pour {validated_url}"
            )
            
        except Exception as executor_error:
            logger.error(f"[{task_id}] ‚ùå Erreur soumission thread: {str(executor_error)}")
            return ScanResponse(
                status="error",
                task_id=task_id,
                message=f"Erreur interne: impossible de d√©marrer l'analyse"
            )
    
    except Exception as e:
        # Gestion d'erreur globale de l'endpoint
        error_task_id = uuid.uuid4().hex[:8]
        logger.exception(f"[{error_task_id}] ‚ùå ERREUR CRITIQUE ENDPOINT /scan: {str(e)}")
        
        return ScanResponse(
            status="error",
            task_id=error_task_id,
            message="Erreur serveur interne"
        )

@app.get("/scan-result/{task_id}")
def get_scan_result(task_id: str) -> Dict[str, Any]:
    """
    R√©cup√®re le r√©sultat d'un scan par son task_id
    Version am√©lior√©e avec gestion d'erreurs
    """
    try:
        logger.info(f"[{task_id}] üìä Demande de r√©sultat")
        
        json_path = REPORTS_DIR / task_id / "result.json"
        
        if not json_path.exists():
            logger.info(f"[{task_id}] ‚è≥ Analyse en cours (pas de r√©sultat)")
            return {
                "status": "en_cours",
                "task_id": task_id,
                "message": "Analyse en cours d'ex√©cution"
            }
        
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                result = json.load(f)
                
            logger.info(f"[{task_id}] ‚úÖ R√©sultat trouv√© et retourn√©")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"[{task_id}] ‚ùå Erreur lecture JSON: {str(e)}")
            return {
                "status": "error",
                "task_id": task_id,
                "error": "Fichier de r√©sultat corrompu",
                "message": "Erreur lors de la lecture du r√©sultat"
            }
            
    except Exception as e:
        logger.exception(f"[{task_id}] ‚ùå Erreur r√©cup√©ration r√©sultat: {str(e)}")
        return {
            "status": "error", 
            "task_id": task_id,
            "error": "Erreur serveur",
            "message": "Erreur lors de la r√©cup√©ration du r√©sultat"
        }

@app.get("/scan-status/{task_id}")
def get_scan_status(task_id: str) -> Dict[str, Any]:
    """
    Endpoint pour v√©rifier uniquement le status d'un scan (plus l√©ger)
    """
    try:
        json_path = REPORTS_DIR / task_id / "result.json"
        
        if not json_path.exists():
            return {"status": "en_cours", "task_id": task_id}
        
        with open(json_path, 'r', encoding='utf-8') as f:
            result = json.load(f)
        
        return {
            "status": result.get("status", "unknown"),
            "task_id": task_id,
            "score": result.get("score", 0),
            "niveau_risque": result.get("niveau_risque", "Inconnu"),
            "nb_failles": len(result.get("failles", [])),
            "duration": result.get("duration_seconds", 0)
        }
        
    except Exception as e:
        logger.error(f"[{task_id}] Erreur status: {str(e)}")
        return {"status": "error", "task_id": task_id}

# Endpoints Stripe inchang√©s (d√©j√† fonctionnels)
@app.post("/create-checkout-session-24h")
def checkout_24h() -> Dict[str, str]:
    try:
        session = stripe.checkout.Session.create(
            payment_method_types=["card"],
            line_items=[{"price": STRIPE_PRICE_24H, "quantity": 1}],
            mode="payment",
            success_url=STRIPE_SUCCESS_URL,
            cancel_url=STRIPE_CANCEL_URL,
        )
        return {"url": session.url}
    except Exception as e:
        logger.exception("Erreur Stripe 24h")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/create-checkout-session-48h")
def checkout_48h() -> Dict[str, str]:
    try:
        session = stripe.checkout.Session.create(
            payment_method_types=["card"],
            line_items=[{"price": STRIPE_PRICE_48H, "quantity": 1}],
            mode="payment",
            success_url=STRIPE_SUCCESS_URL,
            cancel_url=STRIPE_CANCEL_URL,
        )
        return {"url": session.url}
    except Exception as e:
        logger.exception("Erreur Stripe 48h")
        raise HTTPException(status_code=500, detail=str(e))

# Endpoint pour nettoyer les anciens fichiers (optionnel)
@app.post("/admin/cleanup")
def cleanup_old_reports(days: int = 7) -> Dict[str, Any]:
    """
    Nettoie les rapports de plus de X jours (endpoint admin)
    """
    try:
        from datetime import timedelta
        cutoff_date = datetime.utcnow() - timedelta(days=days)
        
        cleaned = 0
        for task_dir in REPORTS_DIR.iterdir():
            if task_dir.is_dir():
                try:
                    dir_time = datetime.fromtimestamp(task_dir.stat().st_mtime)
                    if dir_time < cutoff_date:
                        import shutil
                        shutil.rmtree(task_dir)
                        cleaned += 1
                except Exception:
                    continue
        
        return {"cleaned": cleaned, "days": days}
    except Exception as e:
        logger.error(f"Erreur cleanup: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 10000))
    logger.info(f"üöÄ D√©marrage VELNOR backend v2.0 sur port {port}")
    uvicorn.run("main:app", host="0.0.0.0", port=port)