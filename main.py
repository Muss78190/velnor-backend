import os
import uuid
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import Any, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
from dotenv import load_dotenv
import stripe

from apex_engine import analyse_cybersec
from pdf_generator import generate_pdf_report

# Chargement des variables d'environnement
load_dotenv()

# STRIPE
STRIPE_SECRET_KEY = os.getenv("STRIPE_SECRET_KEY")
STRIPE_PRICE_24H = os.getenv("STRIPE_PRICE_24H", "price_XXX")
STRIPE_PRICE_48H = os.getenv("STRIPE_PRICE_48H", "price_YYY")
STRIPE_SUCCESS_URL = os.getenv("STRIPE_SUCCESS_URL", "https://velnor.fr/success")
STRIPE_CANCEL_URL = os.getenv("STRIPE_CANCEL_URL", "https://velnor.fr/cancel")

# Logger global
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
logger = logging.getLogger("velnor")

# App
app = FastAPI(title="VELNOR API", version="2.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origin_regex=r"https://.*\.?velnor\.fr",
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["Content-Type", "Authorization", "Access-Control-Allow-Origin"]
)

if not STRIPE_SECRET_KEY:
    logger.error("âš ï¸ STRIPE_SECRET_KEY non dÃ©fini dans .env")
stripe.api_key = STRIPE_SECRET_KEY

# RÃ©pertoires
BASE_DIR = Path(__file__).resolve().parent
REPORTS_DIR = BASE_DIR / "rapports"
REPORTS_DIR.mkdir(exist_ok=True)
LOGS_DIR = BASE_DIR / "logs"
LOGS_DIR.mkdir(exist_ok=True)

app.mount("/rapports", StaticFiles(directory=REPORTS_DIR), name="rapports")

class ScanRequest(BaseModel):
    url: str

class ScanResponse(BaseModel):
    status: str
    task_id: str
    message: str

@app.get("/")
def root() -> Dict[str, str]:
    return {"message": "VELNOR backend opÃ©rationnel v2.0", "status": "running"}

@app.get("/health")
def health_check() -> Dict[str, str]:
    """Endpoint de santÃ© pour vÃ©rifier que le service fonctionne"""
    return {"status": "healthy", "timestamp": datetime.utcnow().isoformat()}

# ðŸ†• NOUVELLE ROUTE POUR TÃ‰LÃ‰CHARGER LES PDFs
@app.get("/download-pdf/{task_id}")
async def download_pdf(task_id: str):
    """
    TÃ©lÃ©charge le rapport PDF d'un audit cybersÃ©curitÃ©
    """
    try:
        logger.info(f"[{task_id}] ðŸ“„ Demande de tÃ©lÃ©chargement PDF")
        
        # Chemin du PDF
        pdf_path = REPORTS_DIR / task_id / "audit.pdf"
        
        # VÃ©rifier si le fichier existe
        if not pdf_path.exists():
            logger.error(f"[{task_id}] âŒ PDF non trouvÃ©: {pdf_path}")
            raise HTTPException(status_code=404, detail="Rapport PDF non trouvÃ©")
        
        # Nom du fichier pour le tÃ©lÃ©chargement
        filename = f"velnor_audit_{task_id}.pdf"
        
        logger.info(f"[{task_id}] âœ… PDF trouvÃ©, envoi du fichier")
        
        return FileResponse(
            path=str(pdf_path),
            filename=filename,
            media_type='application/pdf',
            headers={
                "Content-Disposition": f"attachment; filename={filename}",
                "Content-Type": "application/pdf"
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[{task_id}] âŒ Erreur tÃ©lÃ©chargement PDF: {str(e)}")
        raise HTTPException(status_code=500, detail="Erreur lors du tÃ©lÃ©chargement du PDF")

# ðŸ†• ROUTE ALTERNATIVE POUR VÃ‰RIFIER SI LE PDF EXISTE
@app.get("/pdf-ready/{task_id}")
async def check_pdf_ready(task_id: str) -> Dict[str, Any]:
    """
    VÃ©rifie si le PDF est prÃªt pour tÃ©lÃ©chargement
    """
    try:
        pdf_path = REPORTS_DIR / task_id / "audit.pdf"
        
        if pdf_path.exists():
            file_size = pdf_path.stat().st_size
            return {
                "ready": True,
                "task_id": task_id,
                "file_size": file_size,
                "download_url": f"/download-pdf/{task_id}"
            }
        else:
            return {
                "ready": False,
                "task_id": task_id,
                "message": "PDF en cours de gÃ©nÃ©ration"
            }
            
    except Exception as e:
        logger.error(f"[{task_id}] Erreur vÃ©rification PDF: {str(e)}")
        return {
            "ready": False,
            "task_id": task_id,
            "error": str(e)
        }

# ThreadPoolExecutor global avec gestion d'erreurs amÃ©liorÃ©e
executor = ThreadPoolExecutor(max_workers=3, thread_name_prefix="velnor_scan")

def validate_url(url: str) -> tuple[bool, str]:
    """Valide et normalise une URL"""
    try:
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        # Test de connectivitÃ© basique
        resp = requests.head(url, timeout=10, allow_redirects=True)
        if resp.status_code >= 400:
            return False, f"URL retourne status {resp.status_code}"
        
        return True, url
    except requests.exceptions.ConnectionError:
        return False, "Impossible de se connecter Ã  l'URL"
    except requests.exceptions.Timeout:
        return False, "Timeout lors de la connexion"
    except requests.exceptions.InvalidURL:
        return False, "Format d'URL invalide"
    except Exception as e:
        return False, f"Erreur validation URL: {str(e)}"

def execute_cybersec_analysis(url: str, task_id: str, audit_dir: Path) -> None:
    """
    ExÃ©cute l'analyse cybersÃ©curitÃ© dans un thread sÃ©parÃ© avec gestion d'erreurs complÃ¨te
    """
    json_path = audit_dir / "result.json"
    pdf_path = audit_dir / "audit.pdf"
    
    # Logger spÃ©cifique pour cette tÃ¢che
    task_logger = logging.getLogger(f"velnor.task.{task_id}")
    task_logger.setLevel(logging.INFO)
    
    # Ã‰viter les doublons de handlers
    if not task_logger.handlers:
        task_log_path = LOGS_DIR / f"main_logs_{task_id}.txt"
        file_handler = logging.FileHandler(task_log_path)
        formatter = logging.Formatter("%(asctime)s [MAIN] %(levelname)s %(message)s")
        file_handler.setFormatter(formatter)
        task_logger.addHandler(file_handler)
    
    start_time = datetime.utcnow()
    
    try:
        task_logger.info(f"=== DÃ‰BUT THREAD ANALYSE - Task ID: {task_id} ===")
        task_logger.info(f"URL: {url}")
        task_logger.info(f"RÃ©pertoire audit: {audit_dir}")
        
        # Phase 1: Validation URL approfondie
        task_logger.info("Phase 1: Validation URL approfondie...")
        try:
            resp = requests.get(url, timeout=15, verify=False)
            resp.raise_for_status()
            task_logger.info(f"URL accessible - Status: {resp.status_code}")
        except Exception as e:
            error_msg = f"URL non accessible: {str(e)}"
            task_logger.error(error_msg)
            
            # RÃ©sultat d'erreur minimal
            result = {
                "status": "failed",
                "task_id": task_id,
                "url": url,
                "error": error_msg,
                "score": 0,
                "niveau_risque": "Erreur",
                "resume": "âŒ URL inaccessible",
                "anomalies": [error_msg],
                "recommendations": ["VÃ©rifier que l'URL est correcte et accessible"],
                "failles": [],
                "failures": [{"phase": "url_validation", "type": "ConnectionError", "message": error_msg}],
                "phase_results": {"url_validation": {"status": "failed", "error": error_msg}},
                "details_score": {"total": 0},
                "statistiques": {
                    "nb_anomalies": 1,
                    "nb_recommendations": 1,
                    "nb_failles": 0,
                    "nb_failures": 1,
                    "phases_success": 0,
                    "phases_failed": 1,
                    "duree_analyse": (datetime.utcnow() - start_time).total_seconds(),
                    "timestamp": datetime.utcnow().isoformat()
                },
                "metadata": {
                    "version": "2.0",
                    "engine": "velnor_main",
                    "log_file": f"logs/main_logs_{task_id}.txt"
                }
            }
            
            with open(json_path, "w", encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            task_logger.info("RÃ©sultat d'erreur sauvegardÃ©")
            return

        # Phase 2: Analyse cybersÃ©curitÃ© avec notre engine robuste
        task_logger.info("Phase 2: Lancement analyse cybersÃ©curitÃ©...")
        
        # Appel Ã  notre fonction ultra-robuste
        result_data = analyse_cybersec(url, task_id)
        
        task_logger.info(f"Analyse terminÃ©e - Status: {result_data.get('status', 'unknown')}")
        task_logger.info(f"Score obtenu: {result_data.get('score', 0)}/100")
        task_logger.info(f"Failles dÃ©tectÃ©es: {len(result_data.get('failles', []))}")
        
        # Phase 3: GÃ©nÃ©ration PDF
        task_logger.info("Phase 3: GÃ©nÃ©ration du rapport PDF...")
        
        try:
            generate_pdf_report(result_data, str(pdf_path))
            task_logger.info(f"PDF gÃ©nÃ©rÃ© avec succÃ¨s: {pdf_path}")
            pdf_generated = True
        except Exception as e:
            task_logger.error(f"Erreur gÃ©nÃ©ration PDF: {str(e)}")
            pdf_generated = False
            result_data["pdf_error"] = str(e)

        # Phase 4: Finalisation et sauvegarde
        finished_time = datetime.utcnow()
        duration = (finished_time - start_time).total_seconds()
        
        # Enrichissement des mÃ©tadonnÃ©es
        result_data.update({
            "pdf": f"/download-pdf/{task_id}" if pdf_generated else None,  # ðŸ†• Nouvelle URL
            "pdf_generated": pdf_generated,
            "started_at": start_time.isoformat(),
            "finished_at": finished_time.isoformat(),
            "duration_seconds": duration
        })
        
        # Mise Ã  jour des statistiques si elles existent
        if "statistiques" in result_data:
            result_data["statistiques"]["duration_total"] = duration
            result_data["statistiques"]["pdf_generated"] = pdf_generated
        
        # Sauvegarde du rÃ©sultat
        with open(json_path, "w", encoding='utf-8') as f:
            json.dump(result_data, f, indent=2, ensure_ascii=False)
        
        task_logger.info(f"=== ANALYSE TERMINÃ‰E AVEC SUCCÃˆS ===")
        task_logger.info(f"DurÃ©e totale: {duration:.2f}s")
        task_logger.info(f"Fichiers gÃ©nÃ©rÃ©s: JSON âœ“, PDF {'âœ“' if pdf_generated else 'âœ—'}")
        
    except Exception as e:
        # Gestion d'erreur de dernier recours
        task_logger.exception(f"ERREUR CRITIQUE DANS LE THREAD: {str(e)}")
        
        error_result = {
            "status": "error",
            "task_id": task_id,
            "url": url,
            "error": f"Erreur critique: {str(e)}",
            "score": 0,
            "niveau_risque": "Erreur critique",
            "resume": "âŒ Erreur systÃ¨me lors de l'analyse",
            "anomalies": [f"Erreur systÃ¨me: {str(e)}"],
            "recommendations": ["Contacter le support technique"],
            "failles": [],
            "failures": [{"phase": "system", "type": "CriticalError", "message": str(e)}],
            "phase_results": {},
            "details_score": {},
            "statistiques": {
                "nb_anomalies": 1,
                "nb_recommendations": 1,
                "nb_failles": 0,
                "nb_failures": 1,
                "phases_success": 0,
                "phases_failed": 1,
                "duree_analyse": (datetime.utcnow() - start_time).total_seconds(),
                "timestamp": datetime.utcnow().isoformat()
            },
            "metadata": {
                "version": "2.0",
                "engine": "velnor_main_error",
                "log_file": f"logs/main_logs_{task_id}.txt"
            }
        }
        
        try:
            with open(json_path, "w", encoding='utf-8') as f:
                json.dump(error_result, f, indent=2, ensure_ascii=False)
        except Exception as save_error:
            task_logger.error(f"Impossible de sauvegarder le rÃ©sultat d'erreur: {save_error}")
    
    finally:
        # Nettoyage des handlers pour Ã©viter les fuites mÃ©moire
        for handler in task_logger.handlers[:]:
            handler.close()
            task_logger.removeHandler(handler)

@app.post("/scan", response_model=ScanResponse)
def scan_url(data: ScanRequest) -> ScanResponse:
    """
    Endpoint principal pour lancer un audit cybersÃ©curitÃ©
    Version ultra-robuste avec gestion d'erreurs complÃ¨te
    """
    try:
        # GÃ©nÃ©ration task_id unique
        task_id = uuid.uuid4().hex[:8]
        
        logger.info(f"[{task_id}] ðŸ“¥ Nouvelle demande d'audit reÃ§ue")
        logger.info(f"[{task_id}] ðŸŽ¯ URL demandÃ©e: {data.url}")
        
        # Validation prÃ©liminaire de l'URL
        is_valid, validated_url_or_error = validate_url(data.url)
        if not is_valid:
            logger.error(f"[{task_id}] âŒ URL invalide: {validated_url_or_error}")
            return ScanResponse(
                status="error",
                task_id=task_id,
                message=f"URL invalide: {validated_url_or_error}"
            )
        
        validated_url = validated_url_or_error
        logger.info(f"[{task_id}] âœ… URL validÃ©e: {validated_url}")
        
        # CrÃ©ation du rÃ©pertoire d'audit
        audit_dir = REPORTS_DIR / task_id
        audit_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"[{task_id}] ðŸ“ RÃ©pertoire crÃ©Ã©: {audit_dir}")
        
        # Soumission du travail au ThreadPoolExecutor
        try:
            future = executor.submit(execute_cybersec_analysis, validated_url, task_id, audit_dir)
            logger.info(f"[{task_id}] ðŸš€ Thread d'analyse soumis avec succÃ¨s")
            
            return ScanResponse(
                status="en_cours",
                task_id=task_id,
                message=f"Analyse cybersÃ©curitÃ© dÃ©marrÃ©e pour {validated_url}"
            )
            
        except Exception as executor_error:
            logger.error(f"[{task_id}] âŒ Erreur soumission thread: {str(executor_error)}")
            return ScanResponse(
                status="error",
                task_id=task_id,
                message=f"Erreur interne: impossible de dÃ©marrer l'analyse"
            )
    
    except Exception as e:
        # Gestion d'erreur globale de l'endpoint
        error_task_id = uuid.uuid4().hex[:8]
        logger.exception(f"[{error_task_id}] âŒ ERREUR CRITIQUE ENDPOINT /scan: {str(e)}")
        
        return ScanResponse(
            status="error",
            task_id=error_task_id,
            message="Erreur serveur interne"
        )

@app.get("/scan-result/{task_id}")
def get_scan_result(task_id: str) -> Dict[str, Any]:
    """
    RÃ©cupÃ¨re le rÃ©sultat d'un scan par son task_id
    Version amÃ©liorÃ©e avec gestion d'erreurs
    """
    try:
        logger.info(f"[{task_id}] ðŸ“Š Demande de rÃ©sultat")
        
        json_path = REPORTS_DIR / task_id / "result.json"
        
        if not json_path.exists():
            logger.info(f"[{task_id}] â³ Analyse en cours (pas de rÃ©sultat)")
            return {
                "status": "en_cours",
                "task_id": task_id,
                "message": "Analyse en cours d'exÃ©cution"
            }
        
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                result = json.load(f)
                
            logger.info(f"[{task_id}] âœ… RÃ©sultat trouvÃ© et retournÃ©")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"[{task_id}] âŒ Erreur lecture JSON: {str(e)}")
            return {
                "status": "error",
                "task_id": task_id,
                "error": "Fichier de rÃ©sultat corrompu",
                "message": "Erreur lors de la lecture du rÃ©sultat"
            }
            
    except Exception as e:
        logger.exception(f"[{task_id}] âŒ Erreur rÃ©cupÃ©ration rÃ©sultat: {str(e)}")
        return {
            "status": "error", 
            "task_id": task_id,
            "error": "Erreur serveur",
            "message": "Erreur lors de la rÃ©cupÃ©ration du rÃ©sultat"
        }

@app.get("/scan-status/{task_id}")
def get_scan_status(task_id: str) -> Dict[str, Any]:
    """
    Endpoint pour vÃ©rifier uniquement le status d'un scan (plus lÃ©ger)
    """
    try:
        json_path = REPORTS_DIR / task_id / "result.json"
        
        if not json_path.exists():
            return {"status": "en_cours", "task_id": task_id}
        
        with open(json_path, 'r', encoding='utf-8') as f:
            result = json.load(f)
        
        return {
            "status": result.get("status", "unknown"),
            "task_id": task_id,
            "score": result.get("score", 0),
            "niveau_risque": result.get("niveau_risque", "Inconnu"),
            "nb_failles": len(result.get("failles", [])),
            "duration": result.get("duration_seconds", 0)
        }
        
    except Exception as e:
        logger.error(f"[{task_id}] Erreur status: {str(e)}")
        return {"status": "error", "task_id": task_id}

# Endpoints Stripe inchangÃ©s (dÃ©jÃ  fonctionnels)
@app.post("/create-checkout-session-24h")
def checkout_24h() -> Dict[str, str]:
    try:
        session = stripe.checkout.Session.create(
            payment_method_types=["card"],
            line_items=[{"price": STRIPE_PRICE_24H, "quantity": 1}],
            mode="payment",
            success_url=STRIPE_SUCCESS_URL,
            cancel_url=STRIPE_CANCEL_URL,
        )
        return {"url": session.url}
    except Exception as e:
        logger.exception("Erreur Stripe 24h")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/create-checkout-session-48h")
def checkout_48h() -> Dict[str, str]:
    try:
        session = stripe.checkout.Session.create(
            payment_method_types=["card"],
            line_items=[{"price": STRIPE_PRICE_48H, "quantity": 1}],
            mode="payment",
            success_url=STRIPE_SUCCESS_URL,
            cancel_url=STRIPE_CANCEL_URL,
        )
        return {"url": session.url}
    except Exception as e:
        logger.exception("Erreur Stripe 48h")
        raise HTTPException(status_code=500, detail=str(e))

# Endpoint pour nettoyer les anciens fichiers (optionnel)
@app.post("/admin/cleanup")
def cleanup_old_reports(days: int = 7) -> Dict[str, Any]:
    """
    Nettoie les rapports de plus de X jours (endpoint admin)
    """
    try:
        from datetime import timedelta
        cutoff_date = datetime.utcnow() - timedelta(days=days)
        
        cleaned = 0
        for task_dir in REPORTS_DIR.iterdir():
            if task_dir.is_dir():
                try:
                    dir_time = datetime.fromtimestamp(task_dir.stat().st_mtime)
                    if dir_time < cutoff_date:
                        import shutil
                        shutil.rmtree(task_dir)
                        cleaned += 1
                except Exception:
                    continue
        
        return {"cleaned": cleaned, "days": days}
    except Exception as e:
        logger.error(f"Erreur cleanup: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 10000))
    logger.info(f"ðŸš€ DÃ©marrage VELNOR backend v2.0 sur port {port}")
    uvicorn.run("main:app", host="0.0.0.0", port=port)